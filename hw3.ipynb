{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Homework 3\n",
    "## Sentiment analysis using Neural Networks\n",
    "\n",
    "Total: 50 Points\n",
    "\n",
    "\n",
    "In this homework we will perform sentiment analysis using a few simple Neural Network based architectures.\n",
    "For this problem we use the IMDB Large Movie Review Dataset. The dataset contains 25,000 highly polar movie reviews for both train and test dataset, each with 12,500 positive (greater than equal to 7/10 rating) and 12,500 negative reviews(less than equal to 4/10 rating). \n",
    "\n",
    "Use \"https://keras.io/\" for keras documentation. Please use Python 3. GPU is not required but it will help improve the training speed for each problem.\n",
    "\n",
    "Please save the notebook with your cell outputs. You will not be graded if your outputs are not present below the homework cell. Also note your outputs will be unique since you will be using your the last numbers of your uni as your random seed (In the third cell). Make sure you submit this iPython file, with the saved outputs. The submission format must be 'hw3/hw3.ipynb'. You will not submit any other files. If you do save your model weights, you will not submit them. You will however, make sure your model weights do get saved in the 'weights' folder and can be retrieved from there as well.\n",
    "\n",
    "Please fill your details below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Name: Mikael Brunila\n",
    "\n",
    "Uni: tmb2176\n",
    "\n",
    "Email: tmb2176@columbia.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    " # -*- coding: utf-8 -*-\n",
    "\n",
    "from os import listdir\n",
    "import os\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense \n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we retrieve train and test file names\n",
    "\n",
    "train_dir = \"./aclImdb/train/\"\n",
    "test_dir = \"./aclImdb/test/\"\n",
    "tr_review = [re_filename for re_filename in listdir(train_dir)]\n",
    "te_review = [re_filename for re_filename in listdir(test_dir)]\n",
    "\n",
    "#we initialize the train and test arrays\n",
    "\n",
    "tr_X = []\n",
    "tr_Y = []\n",
    "te_X = []\n",
    "te_Y = []\n",
    "\n",
    "#we arrange the reviews into the train and test arrays \n",
    "\n",
    "for review_file in tr_review:\n",
    "    f_review = open(train_dir+review_file,  errors='ignore', mode=\"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    tr_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        tr_Y.append(1)\n",
    "    else:\n",
    "        tr_Y.append(0)\n",
    "        \n",
    "for review_file in te_review:\n",
    "    f_review = open(test_dir+review_file,  errors='ignore', mode=\"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    te_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        te_Y.append(1)\n",
    "    else:\n",
    "        te_Y.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the validation set from the train set\n",
    "\n",
    "use the last 4 numbers of your uni for the seed value seed to ensure all answers remain unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#replace 2 (SEED) with the last 4 numbers of your Uni\n",
    "#Uni: \n",
    "SEED = 2\n",
    "seed_counter = 0\n",
    "while(1):\n",
    "\n",
    "    shuffle_combine = list(zip(tr_X, tr_Y))\n",
    "    random.seed(SEED+seed_counter)\n",
    "    seed_counter+=1\n",
    "    random.shuffle(shuffle_combine)\n",
    "\n",
    "    tr_X, tr_Y = zip(*shuffle_combine)\n",
    "\n",
    "    val_X = tr_X[:5000]\n",
    "    val_Y = tr_Y[:5000]\n",
    "\n",
    "    counter = 0\n",
    "    for label in val_Y:\n",
    "        counter+=label\n",
    "\n",
    "    print (counter)\n",
    "    print (seed_counter)\n",
    "    if(counter>2400 and counter <2600):\n",
    "        tr_X = tr_X[5000:]\n",
    "        tr_Y = tr_Y[5000:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train review set : 20000\n",
      "Length of Train label set : 20000\n",
      "Length of Validation review set : 5000\n",
      "Length of Validation label set : 5000\n",
      "Length of Test review set : 25000\n",
      "Length of Test label set : 25000\n",
      "*****************************************\n",
      "Some sample Reviews Train sets and their labels\n",
      "THIS REVIEW IS MOSTLY ALL SPOILERS. IF YOU PLAN ON ENJOYING THIS FILM, DON'T READ THIS REVIEW.<br /><br /> That's the problem with kids TV nowadays. I\n",
      "0\n",
      "YETI deserves the 8 star rating because it is the one of the greatest bad movies ever made. I saw it at a midnight screening in L.A. and people were r\n",
      "1\n",
      "This movie is a little unusual in that it's got a very slim plot and the movie itself is done at a very slow and leisurely pace. While this makes it p\n",
      "1\n",
      "I keep watching this movie over and over and over. I have to watch it at least once a week. I am from Africa and looking at that movie taught me some \n",
      "1\n",
      "I do not fail to recognize Haneke's above-average film-making skills. For example, I appreciate his lingering on unremarkable-natural-day-lighted sett\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Train review set : \" + str(len(tr_X)))\n",
    "print(\"Length of Train label set : \" + str(len(tr_Y)))\n",
    "print(\"Length of Validation review set : \" + str(len(val_X)))\n",
    "print(\"Length of Validation label set : \" + str(len(val_Y)))\n",
    "print(\"Length of Test review set : \" + str(len(te_X)))\n",
    "print(\"Length of Test label set : \" + str(len(te_Y)))\n",
    "print(\"*****************************************\")\n",
    "print(\"Some sample Reviews Train sets and their labels\")\n",
    "print(tr_X[0][:150])\n",
    "print(tr_Y[0])\n",
    "print(tr_X[1][:150])\n",
    "print(tr_Y[1])\n",
    "print(tr_X[2][:150])\n",
    "print(tr_Y[2])\n",
    "print(tr_X[3][:150])\n",
    "print(tr_Y[3])\n",
    "print(tr_X[4][:150])\n",
    "print(tr_Y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we collect all the reviews from train validation and test set to generate \n",
    "texts = []\n",
    "texts += tr_X \n",
    "texts += te_X \n",
    "texts += val_X\n",
    "len(texts)\n",
    "\n",
    "\n",
    "\n",
    "#we clip the sentence length to first 250 words. \n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#length of vocab, Tokenizer will only use vocab_len most common words\n",
    "vocab_len = 25000\n",
    "\n",
    "#we tokenize the texts and convert all the words to tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_len)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(tr_X)\n",
    "token_te_X = tokenizer.texts_to_sequences(te_X)\n",
    "token_val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "#to ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "#and cut the larger reviews to a max length \n",
    "#(we clip from the top, as the end of the reviews generally have a conclusion which provides better features)\n",
    "x_train = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = sequence.pad_sequences(token_te_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = sequence.pad_sequences(token_val_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#changes the labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(tr_Y)\n",
    "y_test = np_utils.to_categorical(te_Y)\n",
    "y_val = np_utils.to_categorical(val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 250)\n",
      "X_test shape: (25000, 250)\n",
      "X_val shape: (5000, 250)\n",
      "y_train shape: (20000, 2)\n",
      "y_test shape: (25000, 2)\n",
      "y_val shape: (5000, 2)\n",
      "*****************************************\n",
      "Tokenized Reviews Train sets and their labels\n",
      "[ 1990   438 23532   217  1158     4    29     1 18875     2  1094     5\n",
      "   565    10   185   382   621    18    10    77]\n",
      "[ 1.  0.]\n",
      "\n",
      "[   37  1369  4254     2   315  7480   195 12870    82    32  1805  3491\n",
      "  1942     1   219   394     1  1641     6    27]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('X_val shape:', x_val.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"Tokenized Reviews Train sets and their labels\")\n",
    "print(x_train[0][:20])\n",
    "print(y_train[0])\n",
    "print()\n",
    "print(x_train[1][:20])\n",
    "print(y_train[1])\n",
    "print()\n",
    "print(x_train[2][:20])\n",
    "print(y_train[2])\n",
    "print()\n",
    "print(x_train[3][:20])\n",
    "print(y_train[3])\n",
    "print()\n",
    "print(x_train[4][:20])\n",
    "print(y_train[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "\n",
    "As you can see the reviews have now been transformed into indices to tokenized vocabulary and the labels have been converted to one-hot encoding. We can now go ahead and feed these sequences to Neural Network Models.\n",
    "\n",
    "********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "Building your first model (5 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,600,602\n",
      "Trainable params: 9,600,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense \n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_len, input_length = MAX_SEQUENCE_LENGTH, output_dim = 128))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 25s 1ms/step - loss: 0.3974 - acc: 0.8077 - val_loss: 0.2947 - val_acc: 0.8756\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 10s 476us/step - loss: 0.0411 - acc: 0.9865 - val_loss: 0.4519 - val_acc: 0.8476\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 9s 473us/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.5624 - val_acc: 0.8610\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 9s 474us/step - loss: 1.4947e-04 - acc: 1.0000 - val_loss: 0.6007 - val_acc: 0.8614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64a8ebc630>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "Stacking Fully Connected Layers (5 points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,640,802\n",
      "Trainable params: 9,640,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_len, input_length = MAX_SEQUENCE_LENGTH, output_dim = 128))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 10s 501us/step - loss: 0.4069 - acc: 0.7993 - val_loss: 0.2872 - val_acc: 0.8828\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 10s 497us/step - loss: 0.0467 - acc: 0.9847 - val_loss: 0.4460 - val_acc: 0.8470\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 10s 488us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.7219 - val_acc: 0.8540\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 10s 486us/step - loss: 0.0099 - acc: 0.9961 - val_loss: 0.6810 - val_acc: 0.8528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64f0c63710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C\n",
    "\n",
    "Using LSTMS based networks(5 Points) \n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,348,354\n",
      "Trainable params: 3,348,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_len, input_length = MAX_SEQUENCE_LENGTH, output_dim = 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 243s 12ms/step - loss: 0.4278 - acc: 0.8003 - val_loss: 0.3225 - val_acc: 0.8686\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 244s 12ms/step - loss: 0.2147 - acc: 0.9191 - val_loss: 0.3629 - val_acc: 0.8362\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 246s 12ms/step - loss: 0.1180 - acc: 0.9580 - val_loss: 0.3746 - val_acc: 0.8618\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 244s 12ms/step - loss: 0.0851 - acc: 0.9701 - val_loss: 0.4347 - val_acc: 0.8436\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 242s 12ms/step - loss: 0.1075 - acc: 0.9592 - val_loss: 0.6028 - val_acc: 0.8686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6477e5d630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "Adding Pretrained Word Embeddings(10 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122594 unique tokens\n",
      "G Word embeddings: 1917494\n",
      "G Null word embeddings: 34289\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from keras.layers import LSTM\n",
    "\n",
    "#dimension of Glove Embeddings.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#load glove embeddings\n",
    "gembeddings_index = {}\n",
    "with codecs.open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "# nb_words contains the total length of vocab\n",
    "nb_words = len(word_index) +1\n",
    "\n",
    "#get glove embeddings for each word in tokenizer.\n",
    "#g_word_embedding_matrix holds the embeddings dictionary\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "#total words in the tokenizer not in Embedding matrix\n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 250, 300)          36778500  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,014,918\n",
      "Trainable params: 37,014,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = nb_words,\n",
    "                    output_dim = EMBEDDING_DIM, \n",
    "                    weights = [g_word_embedding_matrix], \n",
    "                    input_length = MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(128, recurrent_dropout = 0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 299s 15ms/step - loss: 0.5048 - acc: 0.7443 - val_loss: 0.3179 - val_acc: 0.8724\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 298s 15ms/step - loss: 0.2372 - acc: 0.9123 - val_loss: 0.2618 - val_acc: 0.8920\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 296s 15ms/step - loss: 0.1165 - acc: 0.9622 - val_loss: 0.3073 - val_acc: 0.8996\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 294s 15ms/step - loss: 0.0516 - acc: 0.9850 - val_loss: 0.3677 - val_acc: 0.8912\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 295s 15ms/step - loss: 0.0215 - acc: 0.9941 - val_loss: 0.5258 - val_acc: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63a87f8978>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont attempt this\n",
    "\n",
    "Stacking LSTM layers\n",
    "\n",
    "Unfortunately it takes very long to train, be aware we can stack LTMSs over each other like this.\n",
    "This requires bottom LSTM to return a sequences instead instead of single vector, which becomes input for the top LSTM.\n",
    "\n",
    "\n",
    "![title](img/model5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E\n",
    "\n",
    "Using Convolutional Networks (10 points)\n",
    "\n",
    "Construct the model, shown below. Use the same loss functions and optimizers as before\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model6.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 250, 300)          36778500  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               999552    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,924,454\n",
      "Trainable params: 37,924,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = nb_words,\n",
    "                    output_dim = EMBEDDING_DIM, \n",
    "                    weights = [g_word_embedding_matrix], \n",
    "                    input_length = MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(32, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 0.4508 - acc: 0.7614 - val_loss: 0.2680 - val_acc: 0.8962\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.2240 - acc: 0.9145 - val_loss: 0.3380 - val_acc: 0.8592\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.1318 - acc: 0.9519 - val_loss: 0.2651 - val_acc: 0.9014\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.0752 - acc: 0.9739 - val_loss: 0.3439 - val_acc: 0.8952\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.0415 - acc: 0.9856 - val_loss: 0.4033 - val_acc: 0.8934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63a7afbf60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F\n",
    "\n",
    "Model constructed : (5 points)\n",
    "\n",
    "Test Accuracy Over 87.5%: (5 Points)\n",
    "\n",
    "Bonus: Min(10, Square of (test_score - 88%))\n",
    "\n",
    "Create your best model, use Validation score to judge your best model and check accuracy on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 300)          36778500  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               999552    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,924,454\n",
      "Trainable params: 37,924,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = nb_words,\n",
    "                    output_dim = EMBEDDING_DIM, \n",
    "                    weights = [g_word_embedding_matrix], \n",
    "                    input_length = MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(32, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")\n",
    "\n",
    "model.save_weights(\"./weights/model_best.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep saving models with different names in model_name, \n",
    "\n",
    "so you can retrieve their weights again for testing, you dont have to retrain \n",
    "(You would have to initialize the model definition again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.4268 - acc: 0.7778 - val_loss: 0.2752 - val_acc: 0.8952\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.2221 - acc: 0.9148 - val_loss: 0.2949 - val_acc: 0.8838\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.1277 - acc: 0.9539 - val_loss: 0.2951 - val_acc: 0.8908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa66c61b9e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./weights/\"\n",
    "model_name = 'model_best'\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "bst_model_path = wt_dir + model_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "          shuffle = True,\n",
    "          callbacks=[early_stopping, model_checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on using Ensemble averaging, feel free to edit the code below or add multiple models.\n",
    "\n",
    "Make sure they get saved and can be retrieved when executing serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 139us/step\n",
      "Accuracy: 88.40%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(bst_model_path)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G\n",
    "\n",
    "Explain how Dense, LSTM and Convolution Layers work.\n",
    "\n",
    "Explain Relu, Dropout, and Softmax work.\n",
    "\n",
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took. \n",
    "\n",
    "What are some insights you gained with these experiments? \n",
    "\n",
    "(5 Points)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Dense, LSTM and Convolution Layers \n",
    "\n",
    "Dense vectors function as a replacement for the traditional one-hot vector approach of representing features in a model. Whereas the one-hot vector assigns a dimension to each feature and combination of features, dense vectors work as embeddings and can express a large number of features in a much smaller, d-dimensional space. This gives dense representations a computational advantage. However, the most significant advantage with dense representations of features is that they show relations between similar features, giving them great generalization powers. One-hot vectors are coded as binary values (with zeros except for one one), whereas dense representations are coded as fractional values.\n",
    "\n",
    "Long Short-Term Memory or LSTM is an architecture that is typical to recurrent neural networks. The LSTM architecture splits the state vector s_i of an RNN into two parts, one that functions as a memory cell and the other as working memory. The idea with the memory cell is to preserve memory and error gradients over time. LSTM is a response to the vanishing gradients problem, where gradients as they are multiplied over time approach zero.\n",
    "\n",
    "The purpose of convolution layers, is to integrate the ordering of words into the neural networks. As noted in the Neural Networks textbook, convolutional layers are in essence a feature extracting architecture that is meant to be integrated into a larger network. Unlike recurrent neural networks, which are designed to take into account ordering and are a distinct type of neural network, convolutional neural networks are still part of the feedforward neural network family.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Relu, Dropout and Softmax\n",
    "\n",
    "Relu, or rectified linear unit, is an activation function that performs a nonlinear transformation on the linear preactivation function. It is applied elementwise to the input vectors. Basically, relu transforms each element of a vector or matrix to values of zero or higher (zero if the value is less than zero, higher if it was higher -> relu(x) = max(0,x)). The advantage with relu when calculating gradients, is that the gradient of a positive input is always one (compared to the tanh function, that gives a fractional value as gradient making it approach zero as the amount of gradients increases and are multiplied with each other).\n",
    "\n",
    "The idea of dropout is to prevent the overfitting the model to the data. As explained in the Neural Networkds textbook, there is a danger that multilayer networks learn to rely too much on specific weights. To prevent this from happening, dropout randomly drops a given amount of neurons in a layer. This way, dropout acts as a regularizer and improves generalization on the test data. Dropout makes a bigger difference in deeper networks, but is less relevant in more shallow networks. The fraction of layers to be dropped is given by setting a parameter between 0 and 1 when calling Dropout with Keras.\n",
    "\n",
    "Softmax is an activation function. Much like the sigmoid function, it is usually used at the output layer, giving an output between 0 and 1. For a binary classification tasks, the sigmoid function is good enough, but for multiclass classification we need the softmax function as it extends the sigmoid function to the multiclass case. \n",
    " \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Architectures and insights\n",
    "\n",
    "In A and B, we implement a very basic neural network structure. After using the builtin Keras embedding layer, that is implemented as part of the network in contrast to pre-embeddings, both A and B implement flatten, which according to the Keras documentation takes the dimensions of a vector and multiplies them to give a vector of lower dimensions (for example, a 64 by 32 by 32 matrix would become a 1 by 65536 vector). Next, both A and B implement dense, an elementwise activation function that sets the input and output sizes of the model. Adding more activation functions didn't improve the accuracy of B for me, on the contrary: A has higher accuracy than B. Dense also implements flatten, if the dimensions of the input are less than 2. For both A and B the highest accuracy was achieved during the first epoch.\n",
    "\n",
    "A: Embedding, flatten, dense, relu, dense (2), softmax. Accuracy 0.8614\n",
    "B: Embedding, flatten, dense, relu, dense, relu, dense (2), softmax. Accuracy 0.8528.\n",
    "\n",
    "In C, we move into the territory of RNNs, as we add an LSTM layer to the neural network. This layer seems to replace the step of flattening in A and B, as C is identical to A with the exception of the LSTM layer replacing the flattening layer. We still use the embedding layer of Keras. Accuracy goes up, which I imagine is due to the adding of sequentiality per the RNN architecture.\n",
    "\n",
    "C: Embedding, lstm, dense, relu, dense (2), softmax. Accuracy 0.8686.\n",
    "\n",
    "The models in D and E were implemented using word embeddings pretrained withh GloVe. Like C, D is some form of RNN, as it implements an LSTM layer to increase memory over states (si). I'm not sure that any of the models in A-B work with sequences, which I imagine is one of the reasons that accuracy goes up significantly as we move into D and E, combined with the use of GloVe, which is missing from C. In E, we still use the GloVe pre-embedding, but this time with a convolutional architecture with three layers of 1D convolutions, all associated with layers of dropout and relu, which makes this a significantly deep network compared with the other models. This proves to be the best model in my implementations.\n",
    "\n",
    "D: embedding layer with GloVe, LSTM, dropout, dense, relu, dense, softmax. Accuracy 0.8838.\n",
    "E: embedding layer with GloVe, 3*(convolution 1D, dropout, relu), flatten, dense, dropout, relu, dense, softmax. Accuracy 0.8934.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
